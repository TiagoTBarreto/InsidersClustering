{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import umap.umap_ as umap\n",
    "import pickle\n",
    "import s3fs\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "from scipy.cluster import hierarchy as hc\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn import metrics as mt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_s3 = 's3://insiders-dataset-ttb/'\n",
    "df_raw = pd.read_csv(path_s3 + 'Ecommerce.csv', encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Descrição dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_raw.drop(columns= 'Unnamed: 8').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_new = ['invoice_no', 'stock_code', 'description', 'quantity', 'invoice_date', 'unit_price', 'customer_id','country']\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Dimensions, Types e NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541909, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "invoice_no       object\n",
       "stock_code       object\n",
       "description      object\n",
       "quantity          int64\n",
       "invoice_date     object\n",
       "unit_price      float64\n",
       "customer_id     float64\n",
       "country          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "invoice_no           0\n",
       "stock_code           0\n",
       "description       1454\n",
       "quantity             0\n",
       "invoice_date         0\n",
       "unit_price           0\n",
       "customer_id     135080\n",
       "country              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Replace NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_no</th>\n",
       "      <th>stock_code</th>\n",
       "      <th>description</th>\n",
       "      <th>quantity</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [invoice_no, stock_code, description, quantity, invoice_date, unit_price, customer_id, country]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove na\n",
    "df_missing = df1.loc[df1['customer_id'].isna(), :]\n",
    "df_not_missing = df1.loc[~df1['customer_id'].isna(), :]\n",
    "\n",
    "invoice_number_missing = df_missing.loc[:, 'invoice_no'].tolist()\n",
    "df_not_missing.loc[df_not_missing['invoice_no'].isin(invoice_number_missing), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18287.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_not_missing['customer_id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create reference\n",
    "df_backup = pd.DataFrame(df_missing[['invoice_no']].drop_duplicates())\n",
    "df_backup['customer_id'] = np.arange(19000, 19000 + len(df_backup), 1)\n",
    "\n",
    "# merge with original dataframe\n",
    "df1 = pd.merge(df1, df_backup, on='invoice_no', how='left')\n",
    "\n",
    "# coalesce\n",
    "df1['customer_id'] = df1['customer_id_x'].combine_first(df1['customer_id_y'])\n",
    "\n",
    "# drop columns\n",
    "df1 = df1.drop(columns=['customer_id_x', 'customer_id_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Change Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoice_date\n",
    "df1['invoice_date'] = pd.to_datetime(df1['invoice_date'], format='%d-%b-%y')\n",
    "\n",
    "# customer_id\n",
    "df1['customer_id'] = df1['customer_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes(include = ['int64', 'float64'])\n",
    "cat_attributes = df1.select_dtypes(exclude = ['int64', 'float64', 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>range</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quantity</th>\n",
       "      <td>-80995.00</td>\n",
       "      <td>80995.0</td>\n",
       "      <td>161990.00</td>\n",
       "      <td>9.552250</td>\n",
       "      <td>3.00</td>\n",
       "      <td>218.080957</td>\n",
       "      <td>-0.264076</td>\n",
       "      <td>119769.160031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unit_price</th>\n",
       "      <td>-11062.06</td>\n",
       "      <td>38970.0</td>\n",
       "      <td>50032.06</td>\n",
       "      <td>4.611114</td>\n",
       "      <td>2.08</td>\n",
       "      <td>96.759764</td>\n",
       "      <td>186.506972</td>\n",
       "      <td>59005.719097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <td>12346.00</td>\n",
       "      <td>22709.0</td>\n",
       "      <td>10363.00</td>\n",
       "      <td>16688.840453</td>\n",
       "      <td>16249.00</td>\n",
       "      <td>2911.408666</td>\n",
       "      <td>0.487449</td>\n",
       "      <td>-0.804287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  min      max      range          mean    median  \\\n",
       "quantity    -80995.00  80995.0  161990.00      9.552250      3.00   \n",
       "unit_price  -11062.06  38970.0   50032.06      4.611114      2.08   \n",
       "customer_id  12346.00  22709.0   10363.00  16688.840453  16249.00   \n",
       "\n",
       "                     std        skew       kurtosis  \n",
       "quantity      218.080957   -0.264076  119769.160031  \n",
       "unit_price     96.759764  186.506972   59005.719097  \n",
       "customer_id  2911.408666    0.487449      -0.804287  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Central Tendecy (Mean and Median)\n",
    "c1 = pd.DataFrame(num_attributes.apply(lambda x: np.mean(x)))\n",
    "c2 = pd.DataFrame(num_attributes.apply(lambda x: np.median(x)))\n",
    "\n",
    "# Dispersion\n",
    "d1 = pd.DataFrame(num_attributes.apply(lambda x: (x).min()))\n",
    "d2 = pd.DataFrame(num_attributes.apply(lambda x: (x).max()))\n",
    "d3 = pd.DataFrame(num_attributes.apply(lambda x: (x).max() - (x).min()))\n",
    "d4 = pd.DataFrame(num_attributes.apply(lambda x: np.std(x)))\n",
    "d5 = pd.DataFrame(num_attributes.apply(lambda x: (x).skew()))\n",
    "d6 = pd.DataFrame(num_attributes.apply(lambda x: (x).kurtosis()))\n",
    "\n",
    "a = pd.concat([d1,d2,d3, c1,c2,d4,d5,d6], axis = 1)\n",
    "a.columns = ['min', 'max','range','mean','median','std','skew','kurtosis']\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de invoice number com letras: 9291 e que tem quantidade negativa: 9288\n"
     ]
    }
   ],
   "source": [
    "# invoice_no\n",
    "\n",
    "# df1['invoice_no'] = df1['invoice_no'].astype(int)\n",
    "df_invoice_no = df1.loc[df1['invoice_no'].apply(lambda x: bool(re.search('[^0-9]+', x) ) ), :]\n",
    "quantity_negative = df_invoice_no.loc[df_invoice_no['quantity'] < 0, 'quantity'].count()\n",
    "\n",
    "print(f'Quantidade de invoice number com letras: {df_invoice_no.shape[0]} e que tem quantidade negativa: {quantity_negative}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de stock code para limpar: ['POST', 'D', 'DOT', 'M', 'S', 'AMAZONFEE', 'm', 'DCGSSBOY', 'DCGSSGIRL', 'PADS', 'B', 'CRUK']\n"
     ]
    }
   ],
   "source": [
    "# stock_code\n",
    "\n",
    "# df1['stock_code'] = df1['stock_code'].astype(int)\n",
    "# len(cat_attributes.loc[cat_attributes['stock_code'].apply(lambda x: bool(re.search('[^0-9]+', x) ) ), 'stock_code'].drop_duplicates())\n",
    "\n",
    "stock_code_limpeza = list(cat_attributes.loc[cat_attributes['stock_code'].apply(lambda x: bool(re.search('^[a-zA-Z]+$', x) ) ), 'stock_code'].unique())\n",
    "\n",
    "print(f'Lista de stock code para limpar: {stock_code_limpeza}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['United Kingdom', 'France', 'Australia', 'Netherlands', 'Germany',\n",
       "       'Norway', 'EIRE', 'Switzerland', 'Spain', 'Poland', 'Portugal',\n",
       "       'Italy', 'Belgium', 'Lithuania', 'Japan', 'Iceland',\n",
       "       'Channel Islands', 'Denmark', 'Cyprus', 'Sweden', 'Austria',\n",
       "       'Israel', 'Finland', 'Bahrain', 'Greece', 'Hong Kong', 'Singapore',\n",
       "       'Lebanon', 'United Arab Emirates', 'Saudi Arabia',\n",
       "       'Czech Republic', 'Canada', 'Unspecified', 'Brazil', 'USA',\n",
       "       'European Community', 'Malta', 'RSA'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# country_code\n",
    "cat_attributes.loc[:, 'country'].value_counts(normalize = True)\n",
    "cat_attributes.loc[:, 'country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Filtragem de Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== unit_price ========\n",
    "df2 = df2[df2['unit_price'] >= 0.04]\n",
    "\n",
    "# ======== stock_code ========\n",
    "df2 = df2[~df2['stock_code'].isin(stock_code_limpeza)]\n",
    "\n",
    "# ======== description ========\n",
    "df2 = df2.drop(columns='description', axis = 1)\n",
    " \n",
    "# ======== country ========\n",
    "df2 = df2[~df2['country'].isin(['European Community','Unspecified'])]\n",
    "\n",
    "# ======== bad users ========\n",
    "df2 = df2[~(df2['customer_id'].isin( [16446, 15749] ))]\n",
    "\n",
    "# ======== quantity ========\n",
    "df2_purchases = df2[df2['quantity'] >= 0]\n",
    "df2_returns = df2[df2['quantity'] < 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data reference\n",
    "df_ref = df3.drop(['invoice_no', 'stock_code', 'quantity','invoice_date','unit_price','country'], axis = 1).drop_duplicates(ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Gross Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2237/4216613857.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_purchases.loc[:, 'gross_revenue'] = df2_purchases.loc[:, 'quantity'] * df2_purchases.loc[:, 'unit_price']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "customer_id       0\n",
       "gross_revenue    91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gross Revenue (Faturamento) quantity * price\n",
    "df2_purchases.loc[:, 'gross_revenue'] = df2_purchases.loc[:, 'quantity'] * df2_purchases.loc[:, 'unit_price']\n",
    "\n",
    "# Monetary\n",
    "df_monetary = df2_purchases[['customer_id', 'gross_revenue']].groupby('customer_id').sum().reset_index()\n",
    "df_ref = pd.merge(df_ref, df_monetary, on ='customer_id', how='left')\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Recency Days from the last purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id       0\n",
       "gross_revenue    91\n",
       "recency_days     91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recency - Last day purchase\n",
    "df_recency = df2_purchases[['customer_id','invoice_date']].groupby('customer_id').max().reset_index()\n",
    "df_recency['recency_days'] = (df2_purchases['invoice_date'].max() - df_recency['invoice_date']).dt.days\n",
    "df_recency = df_recency[['customer_id','recency_days']].copy()\n",
    "df_ref = pd.merge(df_ref, df_recency, on ='customer_id', how='left')\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Quantity of purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id       0\n",
       "gross_revenue    91\n",
       "recency_days     91\n",
       "purchases        91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_purch = df2_purchases.loc[:, ['customer_id','invoice_no']].drop_duplicates().groupby('customer_id').count().reset_index().rename(columns={'invoice_no': 'purchases'})\n",
    "df_ref = pd.merge(df_ref, df_purch, on ='customer_id', how='left')\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Quantity of Items purchased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id         0\n",
       "gross_revenue      91\n",
       "recency_days       91\n",
       "purchases          91\n",
       "items_purchased    91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_items_purch = df2_purchases.loc[:, ['customer_id','quantity']].groupby('customer_id').sum().reset_index().rename(columns={'quantity': 'items_purchased'})\n",
    "df_ref = pd.merge(df_ref, df_items_purch, on ='customer_id', how='left')\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Avg Ticket Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id         0\n",
       "gross_revenue      91\n",
       "recency_days       91\n",
       "purchases          91\n",
       "items_purchased    91\n",
       "avg_ticket         91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gross = df2_purchases.loc[:, ['customer_id', 'invoice_no','gross_revenue']].groupby(['customer_id','invoice_no']).sum().reset_index()\n",
    "df_ticket = df_gross.loc[:,['customer_id','gross_revenue']].groupby('customer_id').mean().reset_index().rename(columns={'gross_revenue': 'avg_ticket'})\n",
    "df_ref = pd.merge(df_ref, df_ticket, on ='customer_id', how='left')\n",
    "df_ref.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 Avg Recency Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id            0\n",
       "gross_revenue         91\n",
       "recency_days          91\n",
       "purchases             91\n",
       "items_purchased       91\n",
       "avg_ticket            91\n",
       "avg_recency_days    2816\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aux = df2[['customer_id', 'invoice_date']].drop_duplicates().sort_values(['customer_id', 'invoice_date'], ascending=[True, True])\n",
    "df_aux['next_customer_id'] = df_aux['customer_id'].shift()\n",
    "df_aux['previous_date'] = df_aux['invoice_date'].shift()\n",
    "\n",
    "df_aux['avg_recency_days'] = df_aux.apply(lambda x: (x['invoice_date'] - x['previous_date']).days if x['customer_id'] == x['next_customer_id'] else np.nan, axis = 1)\n",
    "\n",
    "df_aux = df_aux.drop(columns=['invoice_date', 'next_customer_id', 'previous_date'], axis = 1).dropna()\n",
    "\n",
    "df_avg_recency_days = df_aux.groupby('customer_id').mean().reset_index()\n",
    "df_ref = pd.merge(df_ref, df_avg_recency_days, on ='customer_id', how='left')\n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.9 Number of Orders Returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id            0\n",
       "gross_revenue         91\n",
       "recency_days          91\n",
       "purchases             91\n",
       "items_purchased       91\n",
       "avg_ticket            91\n",
       "avg_recency_days    2816\n",
       "number_returns         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Returns\n",
    "df_number_returns = df2_returns.loc[:, ['customer_id','invoice_no']].drop_duplicates().groupby('customer_id').count().reset_index().rename(columns = {'invoice_no': 'number_returns'})\n",
    "df_ref = pd.merge(df_ref, df_number_returns, on ='customer_id', how='left')\n",
    "df_ref.loc[df_ref['number_returns'].isna(), 'number_returns'] = 0  \n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.10 Monetary value of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2237/2122528216.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_returns['monetary_returns'] = df2_returns.loc[:, ['customer_id','quantity','unit_price']].apply(lambda x: x['quantity'] * x['unit_price'] * (-1), axis = 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "customer_id            0\n",
       "gross_revenue         91\n",
       "recency_days          91\n",
       "purchases             91\n",
       "items_purchased       91\n",
       "avg_ticket            91\n",
       "avg_recency_days    2816\n",
       "number_returns         0\n",
       "monetary_returns       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Returns\n",
    "df2_returns['monetary_returns'] = df2_returns.loc[:, ['customer_id','quantity','unit_price']].apply(lambda x: x['quantity'] * x['unit_price'] * (-1), axis = 1)\n",
    "df2_monetary_returns = df2_returns.loc[:, ['customer_id','monetary_returns']].groupby('customer_id').sum().reset_index()\n",
    "df_ref = pd.merge(df_ref, df2_monetary_returns, on ='customer_id', how='left')\n",
    "df_ref.loc[df_ref['monetary_returns'].isna(), 'monetary_returns'] = 0  \n",
    "df_ref.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.13 Delta_buy_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ref['delta_buy_return'] = df_ref['monetary_returns'] / df_ref['gross_revenue'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df_ref.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Estudo do Espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selected = ['items_purchased',\n",
    " 'avg_ticket',\n",
    " 'number_returns',\n",
    " 'monetary_returns',\n",
    " 'delta_buy_return','recency_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df43 = df4.drop(columns=['customer_id'], axis = 1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "mms_gross_revenue = pickle.load(fs.open('s3://insiders-dataset-ttb/mms_gross_revenue.pkl', 'rb'))\n",
    "mms_recency_days = pickle.load(fs.open('s3://insiders-dataset-ttb/mms_recency_days.pkl', 'rb'))\n",
    "mms_items_purchased = pickle.load(fs.open('s3://insiders-dataset-ttb/mms_items_purchased.pkl', 'rb'))\n",
    "mms_avg_ticket = pickle.load(fs.open('s3://insiders-dataset-ttb/mms_avg_ticket.pkl', 'rb'))\n",
    "mms_number_returns = pickle.load(fs.open('s3://insiders-dataset-ttb/mms_number_returns.pkl', 'rb'))\n",
    "mms_monetary_returns = pickle.load(fs.open('s3://insiders-dataset-ttb/mms_monetary_returns.pkl', 'rb'))\n",
    "mms_delta_buy_return = pickle.load(fs.open('s3://insiders-dataset-ttb/mms_delta_buy_return.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df43['gross_revenue'] = mms_gross_revenue.transform(df43[['gross_revenue']])\n",
    "df43['recency_days'] = mms_recency_days.transform(df43[['recency_days']])\n",
    "df43['items_purchased'] = mms_items_purchased.transform(df43[['items_purchased']])\n",
    "df43['avg_ticket'] = mms_avg_ticket.transform(df43[['avg_ticket']])\n",
    "df43['number_returns'] = mms_number_returns.transform(df43[['number_returns']])\n",
    "df43['monetary_returns'] = mms_monetary_returns.transform(df43[['monetary_returns']])\n",
    "df43['delta_buy_return'] = mms_delta_buy_return.transform(df43[['delta_buy_return']])\n",
    "\n",
    "X = df43[feature_selected].copy()\n",
    "y = df4['gross_revenue'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Tree Based Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_from_s3 = pickle.load(fs.open('s3://insiders-dataset-ttb/rf_model.pkl', 'rb'))\n",
    "df_leaf = pd.DataFrame(rf_model_from_s3.apply(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2237/1304610535.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_x'] = embedding[:, 0]\n",
      "/tmp/ipykernel_2237/1304610535.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_y'] = embedding[:, 1]\n",
      "/tmp/ipykernel_2237/1304610535.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_z'] = embedding[:, 2]\n",
      "/tmp/ipykernel_2237/1304610535.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_a'] = embedding[:, 3]\n",
      "/tmp/ipykernel_2237/1304610535.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_b'] = embedding[:, 4]\n",
      "/tmp/ipykernel_2237/1304610535.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_c'] = embedding[:, 5]\n",
      "/tmp/ipykernel_2237/1304610535.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_d'] = embedding[:, 6]\n",
      "/tmp/ipykernel_2237/1304610535.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_e'] = embedding[:, 7]\n",
      "/tmp/ipykernel_2237/1304610535.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_f'] = embedding[:, 8]\n",
      "/tmp/ipykernel_2237/1304610535.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['embedding_g'] = embedding[:, 9]\n"
     ]
    }
   ],
   "source": [
    "reducer = pickle.load(fs.open('s3://insiders-dataset-ttb/reducer.pkl', 'rb'))\n",
    "embedding = reducer.transform(df_leaf)\n",
    "\n",
    "# embedding\n",
    "df_tree = pd.DataFrame()\n",
    "# df_tree\n",
    "df_tree['embedding_x'] = embedding[:, 0]\n",
    "df_tree['embedding_y'] = embedding[:, 1]\n",
    "df_tree['embedding_z'] = embedding[:, 2]\n",
    "df_tree['embedding_a'] = embedding[:, 3]\n",
    "df_tree['embedding_b'] = embedding[:, 4]\n",
    "df_tree['embedding_c'] = embedding[:, 5]\n",
    "df_tree['embedding_d'] = embedding[:, 6]\n",
    "df_tree['embedding_e'] = embedding[:, 7]\n",
    "df_tree['embedding_f'] = embedding[:, 8]\n",
    "df_tree['embedding_g'] = embedding[:, 9]\n",
    "\n",
    "\n",
    "# df4 \n",
    "df4['embedding_x'] = embedding[:, 0]\n",
    "df4['embedding_y'] = embedding[:, 1]\n",
    "df4['embedding_z'] = embedding[:, 2]\n",
    "df4['embedding_a'] = embedding[:, 3]\n",
    "df4['embedding_b'] = embedding[:, 4]\n",
    "df4['embedding_c'] = embedding[:, 5]\n",
    "df4['embedding_d'] = embedding[:, 6]\n",
    "df4['embedding_e'] = embedding[:, 7]\n",
    "df4['embedding_f'] = embedding[:, 8]\n",
    "df4['embedding_g'] = embedding[:, 9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 HyperParameter Fine-Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_tree.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "k = 10\n",
    "\n",
    "hc_model = hc.linkage(X, method='ward')\n",
    "\n",
    "    # model predict\n",
    "labels = hc.fcluster(hc_model, k, criterion='maxclust')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0 Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected = ['customer_id','gross_revenue', 'recency_days', \n",
    " 'items_purchased',\n",
    " 'avg_ticket',\n",
    " 'number_returns', 'purchases'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df4[cols_selected].copy()\n",
    "df9['cluster'] = labels\n",
    "df9['items_purchased'] = df9['items_purchased'].astype(int)\n",
    "df9['recency_days'] = df9['recency_days'].astype(int)\n",
    "df9['number_returns'] = df9['number_returns'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Cluster Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>perc_customer</th>\n",
       "      <th>gross_revenue</th>\n",
       "      <th>recency_days</th>\n",
       "      <th>items_purchased</th>\n",
       "      <th>number_returns</th>\n",
       "      <th>avg_ticket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elite Insiders</td>\n",
       "      <td>291</td>\n",
       "      <td>9.81</td>\n",
       "      <td>14985.14</td>\n",
       "      <td>21.13</td>\n",
       "      <td>9055.00</td>\n",
       "      <td>4.09</td>\n",
       "      <td>775.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top Consumers</td>\n",
       "      <td>175</td>\n",
       "      <td>5.90</td>\n",
       "      <td>3901.97</td>\n",
       "      <td>27.13</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>1.69</td>\n",
       "      <td>419.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potential</td>\n",
       "      <td>365</td>\n",
       "      <td>12.30</td>\n",
       "      <td>2592.67</td>\n",
       "      <td>40.54</td>\n",
       "      <td>1588.74</td>\n",
       "      <td>1.31</td>\n",
       "      <td>383.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Promising</td>\n",
       "      <td>331</td>\n",
       "      <td>11.16</td>\n",
       "      <td>1799.58</td>\n",
       "      <td>44.69</td>\n",
       "      <td>1057.24</td>\n",
       "      <td>0.98</td>\n",
       "      <td>343.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sporadic Consumers</td>\n",
       "      <td>242</td>\n",
       "      <td>8.16</td>\n",
       "      <td>1409.87</td>\n",
       "      <td>49.10</td>\n",
       "      <td>790.42</td>\n",
       "      <td>0.76</td>\n",
       "      <td>310.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Risky Segment</td>\n",
       "      <td>326</td>\n",
       "      <td>10.99</td>\n",
       "      <td>1070.53</td>\n",
       "      <td>61.33</td>\n",
       "      <td>589.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>284.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need Attention</td>\n",
       "      <td>252</td>\n",
       "      <td>8.49</td>\n",
       "      <td>842.95</td>\n",
       "      <td>70.87</td>\n",
       "      <td>441.69</td>\n",
       "      <td>0.57</td>\n",
       "      <td>259.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>At Risk</td>\n",
       "      <td>512</td>\n",
       "      <td>17.26</td>\n",
       "      <td>592.36</td>\n",
       "      <td>86.88</td>\n",
       "      <td>293.97</td>\n",
       "      <td>0.49</td>\n",
       "      <td>220.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Churn Group</td>\n",
       "      <td>237</td>\n",
       "      <td>7.99</td>\n",
       "      <td>431.12</td>\n",
       "      <td>132.49</td>\n",
       "      <td>144.71</td>\n",
       "      <td>0.62</td>\n",
       "      <td>231.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hibernating</td>\n",
       "      <td>236</td>\n",
       "      <td>7.95</td>\n",
       "      <td>237.94</td>\n",
       "      <td>103.94</td>\n",
       "      <td>111.90</td>\n",
       "      <td>0.31</td>\n",
       "      <td>97.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cluster  customer_id  perc_customer  gross_revenue  \\\n",
       "0      Elite Insiders          291           9.81       14985.14   \n",
       "1       Top Consumers          175           5.90        3901.97   \n",
       "2           Potential          365          12.30        2592.67   \n",
       "3           Promising          331          11.16        1799.58   \n",
       "4  Sporadic Consumers          242           8.16        1409.87   \n",
       "5       Risky Segment          326          10.99        1070.53   \n",
       "6      Need Attention          252           8.49         842.95   \n",
       "7             At Risk          512          17.26         592.36   \n",
       "8         Churn Group          237           7.99         431.12   \n",
       "9         Hibernating          236           7.95         237.94   \n",
       "\n",
       "   recency_days  items_purchased  number_returns  avg_ticket  \n",
       "0         21.13          9055.00            4.09      775.09  \n",
       "1         27.13          2325.30            1.69      419.70  \n",
       "2         40.54          1588.74            1.31      383.90  \n",
       "3         44.69          1057.24            0.98      343.52  \n",
       "4         49.10           790.42            0.76      310.74  \n",
       "5         61.33           589.20            0.78      284.66  \n",
       "6         70.87           441.69            0.57      259.37  \n",
       "7         86.88           293.97            0.49      220.57  \n",
       "8        132.49           144.71            0.62      231.69  \n",
       "9        103.94           111.90            0.31       97.49  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of customers\n",
    "df_cluster = df9[['customer_id','cluster']].groupby('cluster').count().reset_index()\n",
    "df_cluster['perc_customer'] = 100* (df_cluster['customer_id'] / df_cluster['customer_id'].sum() )\n",
    "\n",
    "# Avg gross_revenue\n",
    "df_gross = df9[['gross_revenue','cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_gross, on='cluster', how='left')\n",
    "\n",
    "\n",
    "# Avg recency_days\n",
    "df_recency_days = df9[['recency_days','cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_recency_days, on='cluster', how='left')\n",
    "\n",
    "# Avg purchases\n",
    "df_purchases = df9[['purchases','cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_purchases, on='cluster', how='left')\n",
    "\n",
    "# Avg items_purchased\n",
    "df_items_purchased = df9[['items_purchased','cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_items_purchased, on='cluster', how='left')\n",
    "\n",
    "# Avg number_returns\n",
    "df_number_returns = df9[['number_returns','cluster']].groupby('cluster').mean().reset_index()\n",
    "df_cluster = pd.merge(df_cluster, df_number_returns, on='cluster', how='left')\n",
    "\n",
    "\n",
    "\n",
    "df_cluster['avg_ticket'] = df_cluster.apply(lambda x: x['gross_revenue'] / x['purchases'], axis = 1)\n",
    "df_cluster = df_cluster.drop('purchases', axis = 1)\n",
    "df_cluster = np.round(df_cluster, 2)\n",
    "\n",
    "\n",
    "cluster_names = {\n",
    "    10: \"Elite Insiders\",\n",
    "    9: \"Top Consumers\",\n",
    "    8: \"Potential\",\n",
    "    5: \"Promising\",\n",
    "    6: \"Sporadic Consumers\",\n",
    "    7: \"Risky Segment\",\n",
    "    1: \"Need Attention\",\n",
    "    2: \"At Risk\",\n",
    "    4: \"Churn Group\",\n",
    "    3: \"Hibernating\"\n",
    "}\n",
    "\n",
    "df_cluster['cluster'] = df_cluster['cluster'].map(cluster_names)\n",
    "\n",
    "\n",
    "df_cluster = df_cluster.sort_values('gross_revenue', ascending = False).reset_index(drop = True)\n",
    "\n",
    "df_cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster 10: Elite Insiders\n",
    "- Cluster 9: Top Consumers\n",
    "- Cluster 8: Potential\n",
    "- Cluster 5: Promising\n",
    "- Cluster 6 Sporadical Consumers\n",
    "- Cluster 7: Risky Segment\n",
    "- Cluster 1: Need Atention\n",
    "- Cluster 2: At Risk\n",
    "- Cluster 4: Churn Group\n",
    "- Cluster 3: Hibernating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0 Deploy To Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = df9.drop(columns='purchases').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Insert into SQLITe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection\n",
    "engine = create_engine('postgresql://meigarom:comunidadeds!@db-insiders.cha40cm68lvi.us-east-1.rds.amazonaws.com:5433/postgres')\n",
    "\n",
    "conn = engine.connect()\n",
    "\n",
    "# insert data\n",
    "df10.to_sql('insiders', con = conn, if_exists= 'append', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
